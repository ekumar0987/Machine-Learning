{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekumar0987/Machine-Learning/blob/main/Stock_Closing_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "385a1e84",
      "metadata": {
        "id": "385a1e84"
      },
      "source": [
        "### Business Problem\n",
        "\n",
        "Develop a predictive model to forecast the closing price of a stock on any given day, facilitating data-driven financial analysis for portfolio optimization, risk assessment, and strategy development in algorithmic trading systems. The goal is to leverage historical price and volume data to generate accurate, timely predictions that can support investment decisions and enhance market responsiveness."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2764240e",
      "metadata": {
        "id": "2764240e"
      },
      "source": [
        "### Data Extraction\n",
        "\n",
        "##### Unzip and load the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "pwewar7kGCC6"
      },
      "id": "pwewar7kGCC6",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except Exception as e:\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXoWaMKrDP6U",
        "outputId": "b665f4cd-1ad7-4e91-ec24-a58e065aecaa"
      },
      "id": "JXoWaMKrDP6U",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2dea799",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2dea799",
        "outputId": "c4c1ac95-aace-48a1-9a1b-01a092b7f704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory:  /content\n",
            "New Working Directory:  /content/drive/My Drive/Pinky Personal/Data Science/Data\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Current Working Directory: \", os.getcwd())\n",
        "os.chdir('/content/drive/My Drive/Pinky Personal/Data Science/Data')\n",
        "print(\"New Working Directory: \", os.getcwd())\n",
        "\n",
        "zip_file_path = 'archive.zip'\n",
        "extract_to_dir = '.'\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52a523a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52a523a4",
        "outputId": "befd86af-a5f4-424f-f538-276d0eb658f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['archive.zip', 'ETFs', 'Stocks']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir('/content/drive/My Drive/Pinky Personal/Data Science/Data'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3397839e",
      "metadata": {
        "id": "3397839e"
      },
      "source": [
        "##### Create a unified financial dataset\n",
        "Read data from each stock/ETF file and merge it into a single dataset. Augment data with ticker and type information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6b01a47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6b01a47",
        "outputId": "319f2396-e451-4311-99bf-9d132f008d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Skipping empty file: stnl.us.txt\n",
            "Skipping empty file: vist.us.txt\n",
            "Skipping empty file: vmet.us.txt\n",
            "Skipping empty file: wnfm.us.txt\n",
            "Skipping empty file: wspt.us.txt\n",
            "Skipping empty file: pxus.us.txt\n",
            "Skipping empty file: rbio.us.txt\n",
            "Skipping empty file: sail.us.txt\n",
            "Skipping empty file: sbt.us.txt\n",
            "Skipping empty file: scci.us.txt\n",
            "Skipping empty file: scph.us.txt\n",
            "Skipping empty file: send.us.txt\n",
            "Skipping empty file: sfix.us.txt\n",
            "Skipping empty file: srva.us.txt\n",
            "Skipping empty file: molc.us.txt\n",
            "Skipping empty file: otg.us.txt\n",
            "Skipping empty file: pbio.us.txt\n",
            "Skipping empty file: jt.us.txt\n",
            "Skipping empty file: mapi.us.txt\n",
            "Skipping empty file: fmax.us.txt\n",
            "Skipping empty file: gnst.us.txt\n",
            "Skipping empty file: hayu.us.txt\n",
            "Skipping empty file: ehr.us.txt\n",
            "Skipping empty file: amrh.us.txt\n",
            "Skipping empty file: amrhw.us.txt\n",
            "Skipping empty file: asns.us.txt\n",
            "Skipping empty file: bbrx.us.txt\n",
            "Skipping empty file: bolt.us.txt\n",
            "Skipping empty file: boxl.us.txt\n",
            "Skipping empty file: bxg.us.txt\n",
            "Skipping empty file: accp.us.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "print(os.getcwd())\n",
        "os.chdir('/content/drive/My Drive/Pinky Personal/Data Science/Data')\n",
        "\n",
        "def load_data_from_directory(directory, data_type):\n",
        "    all_data = []\n",
        "    for filename in os.listdir(directory):\n",
        "      if filename.endswith('.txt'):\n",
        "          file_path = os.path.join(directory, filename)\n",
        "          if os.stat(file_path).st_size == 0:  # Check if the file is empty\n",
        "              print(f\"Skipping empty file: {filename}\")\n",
        "              continue\n",
        "          stock_data = pd.read_csv(file_path)\n",
        "          ticker = filename.split('/')[-1].replace('.us.txt', '')\n",
        "          stock_data['Ticker'] = ticker\n",
        "          stock_data['Type'] = data_type  # Add type column (Stock or ETF)\n",
        "          all_data.append(stock_data)\n",
        "\n",
        "    return pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "stocks_data = load_data_from_directory('/content/drive/My Drive/Pinky Personal/Data Science/Data/Stocks', 'Stock')\n",
        "etfs_data = load_data_from_directory('/content/drive/My Drive/Pinky Personal/Data Science/Data/ETFs', 'ETF')\n",
        "\n",
        "# Combine the stocks and ETFs data into a single DataFrame\n",
        "combined_data = pd.concat([stocks_data, etfs_data], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80fedf0b",
      "metadata": {
        "id": "80fedf0b"
      },
      "source": [
        "##### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a24e9620",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a24e9620",
        "outputId": "73988ad8-83cc-4872-ece0-b5afb0e022cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date  Open  High   Low  Close  Volume  OpenInt Ticker   Type\n",
            "0  2005-02-25  6.34  6.44  6.34   6.41    3200        0    ssy  Stock\n",
            "1  2005-02-28  6.43  6.58  6.43   6.58    1500        0    ssy  Stock\n",
            "2  2005-03-01  6.68  6.98  6.68   6.89    7200        0    ssy  Stock\n",
            "3  2005-03-02  6.90  6.92  6.90   6.92    1500        0    ssy  Stock\n",
            "4  2005-03-03  6.90  6.97  6.85   6.91    3000        0    ssy  Stock\n",
            "               Open          High           Low         Close        Volume  \\\n",
            "count  1.735116e+07  1.735116e+07  1.735116e+07  1.735116e+07  1.735116e+07   \n",
            "mean   2.640325e+04  2.712965e+04  2.551097e+04  2.628191e+04  1.584260e+06   \n",
            "std    3.893321e+06  4.005433e+06  3.749236e+06  3.873026e+06  8.424769e+06   \n",
            "min    0.000000e+00  4.000000e-03 -1.000000e+00  3.700000e-03  0.000000e+00   \n",
            "25%    8.877900e+00  9.007400e+00  8.731300e+00  8.876200e+00  2.572600e+04   \n",
            "50%    1.837600e+01  1.862100e+01  1.811300e+01  1.837600e+01  1.573000e+05   \n",
            "75%    3.366000e+01  3.406000e+01  3.323200e+01  3.366000e+01  7.838850e+05   \n",
            "max    1.423713e+09  1.442049e+09  1.362118e+09  1.437986e+09  2.423735e+09   \n",
            "\n",
            "          OpenInt  \n",
            "count  17351164.0  \n",
            "mean          0.0  \n",
            "std           0.0  \n",
            "min           0.0  \n",
            "25%           0.0  \n",
            "50%           0.0  \n",
            "75%           0.0  \n",
            "max           0.0  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17351164 entries, 0 to 17351163\n",
            "Data columns (total 9 columns):\n",
            " #   Column   Dtype  \n",
            "---  ------   -----  \n",
            " 0   Date     object \n",
            " 1   Open     float64\n",
            " 2   High     float64\n",
            " 3   Low      float64\n",
            " 4   Close    float64\n",
            " 5   Volume   int64  \n",
            " 6   OpenInt  int64  \n",
            " 7   Ticker   object \n",
            " 8   Type     object \n",
            "dtypes: float64(4), int64(2), object(3)\n",
            "memory usage: 1.2+ GB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "print(combined_data.head())\n",
        "\n",
        "print(combined_data.describe())\n",
        "\n",
        "print(combined_data.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fed6be4",
      "metadata": {
        "id": "1fed6be4"
      },
      "source": [
        "##### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea3e7b9b",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea3e7b9b",
        "outputId": "edfd2a8e-d4a1-4b0a-a3f7-63a8547ae9b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Open             0\n",
            "High             0\n",
            "Low              0\n",
            "Volume           0\n",
            "Year             0\n",
            "Month            0\n",
            "Day              0\n",
            "Type_ETF         0\n",
            "Type_Stock       0\n",
            "TickerEncoded    0\n",
            "dtype: int64\n",
            "Open             0\n",
            "High             0\n",
            "Low              0\n",
            "Volume           0\n",
            "Year             0\n",
            "Month            0\n",
            "Day              0\n",
            "Type_ETF         0\n",
            "Type_Stock       0\n",
            "TickerEncoded    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore', message='is_sparse is deprecated and will be removed in a future version.*')\n",
        "\n",
        "sampled_data = combined_data.sample(frac=0.1, random_state=42)\n",
        "\n",
        "sampled_data['Datetime'] = pd.to_datetime(sampled_data['Date'])\n",
        "sampled_data.sort_values(by='Datetime', inplace=True)\n",
        "sampled_data['Year'] = sampled_data['Datetime'].dt.year\n",
        "sampled_data['Month'] = sampled_data['Datetime'].dt.month\n",
        "sampled_data['Day'] = sampled_data['Datetime'].dt.day\n",
        "\n",
        "# Encode 'Type' using one-hot encoding\n",
        "sampled_data = pd.get_dummies(sampled_data, columns=['Type'], dtype=int)\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "sampled_data['TickerEncoded'] = label_encoder.fit_transform(sampled_data['Ticker'])\n",
        "\n",
        "# Now drop the original 'Ticker' column\n",
        "sampled_data.drop('Ticker', axis=1, inplace=True)\n",
        "\n",
        "# Define y and X from sampled_data\n",
        "y = sampled_data['Close']\n",
        "X = sampled_data.drop(['Close', 'Date', 'Datetime', 'OpenInt'], axis=1)\n",
        "\n",
        "# Define columns to scale (excluding 'Year', 'Month', 'Day', 'TickerEncoded')\n",
        "columns_to_scale = ['Open', 'High', 'Low', 'Volume']\n",
        "\n",
        "# Initialize StandardScaler for features and target\n",
        "scaler_features = StandardScaler()\n",
        "scaler_target = StandardScaler()\n",
        "\n",
        "# Prepare lists to store train and test data\n",
        "train_dfs = []\n",
        "test_dfs = []\n",
        "\n",
        "# Define the maximum number of splits for TimeSeriesSplit\n",
        "max_splits = 5\n",
        "\n",
        "# Initialize the TimeSeriesSplit with a dynamic split number\n",
        "tscv = TimeSeriesSplit(n_splits=max_splits)\n",
        "\n",
        "# Group by 'TickerEncoded' and perform time series split\n",
        "for ticker, group_data in X.groupby('TickerEncoded'):\n",
        "    y_group = y[group_data.index].reset_index(drop=True)\n",
        "    group_data = group_data.reset_index(drop=True)\n",
        "\n",
        "    # Adjusting n_splits to ensure at least 2 splits if the group size allows\n",
        "    if len(group_data) > 2:  # Check if group size allows for at least one split\n",
        "        n_splits = min(max_splits, len(group_data) - 1)\n",
        "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "        for train_index, test_index in tscv.split(group_data):\n",
        "            # Split into train and test sets\n",
        "            X_train_grp, X_test_grp = group_data.iloc[train_index], group_data.iloc[test_index]\n",
        "            y_train_grp, y_test_grp = y_group.iloc[train_index], y_group.iloc[test_index]\n",
        "\n",
        "            # Scale features\n",
        "            X_train_grp_scaled = scaler_features.fit_transform(X_train_grp[columns_to_scale])\n",
        "            X_test_grp_scaled = scaler_features.transform(X_test_grp[columns_to_scale])\n",
        "\n",
        "            # Scale target\n",
        "            y_train_grp_scaled = scaler_target.fit_transform(y_train_grp.values.reshape(-1, 1)).flatten()\n",
        "            y_test_grp_scaled = scaler_target.transform(y_test_grp.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "            X_train_grp = pd.concat([pd.DataFrame(X_train_grp_scaled, columns=columns_to_scale),\n",
        "                                  X_train_grp.drop(columns=columns_to_scale).reset_index(drop=True)], axis=1)\n",
        "            X_test_grp = pd.concat([pd.DataFrame(X_test_grp_scaled, columns=columns_to_scale),\n",
        "                                 X_test_grp.drop(columns=columns_to_scale).reset_index(drop=True)], axis=1)\n",
        "\n",
        "            # Append to the lists\n",
        "            train_dfs.append((X_train_grp, y_train_grp_scaled))\n",
        "            test_dfs.append((X_test_grp, y_test_grp_scaled))\n",
        "\n",
        "    else:\n",
        "        # Handle groups with 2 or fewer samples, perhaps by adding them entirely to training or testing\n",
        "        pass\n",
        "\n",
        "# Concatenate all training and testing groups\n",
        "X_train, y_train = zip(*train_dfs)\n",
        "X_train = pd.concat(X_train, ignore_index=True)\n",
        "y_train = np.concatenate(y_train, axis=0)\n",
        "\n",
        "X_test, y_test = zip(*test_dfs)\n",
        "X_test = pd.concat(X_test, ignore_index=True)\n",
        "y_test = np.concatenate(y_test, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74adde1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "b74adde1",
        "outputId": "017afbec-7b9a-4365-cad6-40580ad19563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
            "Best R2 from Randomized Search: 0.9952126144966934\n",
            "Best Parameters: {'colsample_bytree': 0.9977829850443283, 'gamma': 0.08796262633867269, 'learning_rate': 0.013615072723104174, 'max_depth': 8, 'min_child_weight': 5, 'n_estimators': 865, 'subsample': 0.8985965620472096}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'mean_test_r2_score'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-92a82b35f080>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mcv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_random_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mmean_test_r2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_r2_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_test_r2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mmean_test_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_neg_mean_squared_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'mean_test_r2_score'"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBRegressor\n",
        "from scipy.stats import uniform, randint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define a parameter grid to search\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 1000),\n",
        "    'max_depth': randint(3, 10),\n",
        "    'learning_rate': uniform(0.01, 0.2),\n",
        "    'subsample': uniform(0.6, 0.4),\n",
        "    'colsample_bytree': uniform(0.6, 0.4),\n",
        "    'min_child_weight': randint(1, 10),\n",
        "    'gamma': uniform(0, 0.5),\n",
        "}\n",
        "\n",
        "# Initialize the XGBRegressor\n",
        "xgb = XGBRegressor(random_state=42)\n",
        "\n",
        "# Initialize the RandomizedSearchCV object\n",
        "xgb_random_search = RandomizedSearchCV(\n",
        "    xgb, param_distributions=param_dist,\n",
        "    n_iter=100,\n",
        "    scoring='r2',\n",
        "    cv=3,\n",
        "    verbose=3,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the data\n",
        "xgb_random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and the corresponding R2\n",
        "best_score = xgb_random_search.best_score_\n",
        "best_params = xgb_random_search.best_params_\n",
        "print(f\"Best R2 from Randomized Search: {best_score}\")\n",
        "print(f\"Best Parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, r2_score, mean_squared_error\n",
        "\n",
        "# Define your parameter grid and model as before\n",
        "\n",
        "# Create scorers dictionary\n",
        "scorers = {\n",
        "    'r2': make_scorer(r2_score),\n",
        "    'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False)\n",
        "}\n",
        "\n",
        "# Pass the scorers dictionary to the scoring parameter\n",
        "random_search = RandomizedSearchCV(estimator=xgb,\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=100,\n",
        "                                   scoring=scorers,\n",
        "                                   refit='r2',  # Metric to refit the model on\n",
        "                                   cv=3,\n",
        "                                   verbose=3,\n",
        "                                   random_state=42,\n",
        "                                   n_jobs=-1,\n",
        "                                   return_train_score=True)\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# After fitting, the scores are stored in the cv_results_ attribute\n",
        "cv_results = random_search.cv_results_\n",
        "\n",
        "# Access the mean test score for each metric\n",
        "mean_test_r2 = cv_results['mean_test_r2']\n",
        "mean_test_neg_mse = cv_results['mean_test_neg_mean_squared_error']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE48FmX7seJH",
        "outputId": "9b0294a4-1c69-40fe-b471-047f63051b97"
      },
      "id": "oE48FmX7seJH",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean_test_r2)\n",
        "print(mean_test_neg_mse)\n",
        "\n",
        "# cv_results = xgb_random_search.cv_results_\n",
        "# print(cv_results)\n",
        "\n",
        "# mean_test_r2 = cv_results['mean_test_r2_score']\n",
        "# print(mean_test_r2)\n",
        "# mean_test_mse = -cv_results['mean_test_neg_mean_squared_error']\n",
        "# print(mean_test_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1RZJywvhZws",
        "outputId": "26d23bc2-2974-4bfe-ffa4-c83ec055c407"
      },
      "id": "a1RZJywvhZws",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.99509449 0.99509335 0.99518175 0.99481417 0.99505168 0.99509227\n",
            " 0.99505501 0.99510916 0.99513346 0.99513669 0.9951675  0.99504084\n",
            " 0.9949353  0.99520398 0.99518487 0.99513593 0.99509822 0.99381216\n",
            " 0.99517547 0.99506337 0.99511101 0.99515137 0.99514492 0.99482451\n",
            " 0.99495084 0.99508358 0.99512162 0.99514112 0.9951184  0.99489773\n",
            " 0.99504532 0.99504595 0.99517204 0.99506776 0.99517121 0.99521261\n",
            " 0.9948856  0.99513352 0.994964   0.99514454 0.99514154 0.99506231\n",
            " 0.99499872 0.9948908  0.99512752 0.9949987  0.99475509 0.99513459\n",
            " 0.99510017 0.99507356 0.99509993 0.99500887 0.99460645 0.99518838\n",
            " 0.99502009 0.99511398 0.99428622 0.99502083 0.99515396 0.9950961\n",
            " 0.99498995 0.99497181 0.99510206 0.99511983 0.99513005 0.99317992\n",
            " 0.99511873 0.99500711 0.99512305 0.99508053 0.99493254 0.99516094\n",
            " 0.99513886 0.99516008 0.99512166 0.99516964 0.99493026 0.99514515\n",
            " 0.99502689 0.99511979 0.99502395 0.99509888 0.99485655 0.99516449\n",
            " 0.99520282 0.99504365 0.99509921 0.99508706 0.99511504 0.99514881\n",
            " 0.99448903 0.99515815 0.99490674 0.99513861 0.99516082 0.99491955\n",
            " 0.99514467 0.99515242 0.99506387 0.99518134]\n",
            "[-0.0049052  -0.00490634 -0.00481795 -0.00518551 -0.00494801 -0.00490742\n",
            " -0.00494469 -0.00489053 -0.00486623 -0.00486301 -0.0048322  -0.00495884\n",
            " -0.00506438 -0.00479572 -0.00481483 -0.00486376 -0.00490147 -0.00618745\n",
            " -0.00482423 -0.00493633 -0.00488868 -0.00484833 -0.00485478 -0.00517516\n",
            " -0.00504884 -0.00491611 -0.00487808 -0.00485858 -0.0048813  -0.00510195\n",
            " -0.00495437 -0.00495374 -0.00482765 -0.00493193 -0.00482848 -0.00478709\n",
            " -0.00511408 -0.00486618 -0.00503568 -0.00485515 -0.00485815 -0.00493738\n",
            " -0.00500096 -0.00510887 -0.00487217 -0.00500099 -0.00524458 -0.00486511\n",
            " -0.00489952 -0.00492613 -0.00489976 -0.00499082 -0.00539321 -0.00481131\n",
            " -0.00497959 -0.00488571 -0.00571342 -0.00497886 -0.00484574 -0.00490359\n",
            " -0.00500973 -0.00502787 -0.00489763 -0.00487986 -0.00486964 -0.00681965\n",
            " -0.00488097 -0.00499257 -0.00487664 -0.00491917 -0.00506714 -0.00483876\n",
            " -0.00486083 -0.00483962 -0.00487804 -0.00483006 -0.00506942 -0.00485454\n",
            " -0.0049728  -0.00487991 -0.00497574 -0.00490081 -0.00514313 -0.00483521\n",
            " -0.00479688 -0.00495604 -0.00490048 -0.00491264 -0.00488465 -0.00485089\n",
            " -0.00551062 -0.00484154 -0.00509294 -0.00486108 -0.00483888 -0.00508014\n",
            " -0.00485502 -0.00484728 -0.00493582 -0.00481835]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract the mean test scores for each cross-validation fold\n",
        "mean_test_scores = xgb_random_search.cv_results_['mean_test_score']\n",
        "mean_train_scores = xgb_random_search.cv_results_['mean_train_score']\n",
        "\n",
        "# Extract the number of estimators for each iteration of the RandomizedSearchCV\n",
        "# Note: This assumes 'n_estimators' was a parameter being tuned.\n",
        "n_estimators = [params['n_estimators'] for params in xgb_random_search.cv_results_['params']]\n",
        "\n",
        "# Sort the scores by the number of estimators for plotting\n",
        "sorted_indices = np.argsort(n_estimators)\n",
        "n_estimators_sorted = np.array(n_estimators)[sorted_indices]\n",
        "mean_test_scores_sorted = np.array(mean_test_scores)[sorted_indices]\n",
        "mean_train_scores_sorted = np.array(mean_train_scores)[sorted_indices]\n",
        "\n",
        "# Plot the learning curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(n_estimators_sorted, mean_train_scores_sorted, label='Train Score')\n",
        "plt.plot(n_estimators_sorted, mean_test_scores_sorted, label='Test Score', linestyle='--')\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('R2 Score')\n",
        "plt.title('Learning Curves')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "KnYYG38ZB3W_",
        "outputId": "090bdf5a-55a0-47e7-90d6-032469d32215"
      },
      "id": "KnYYG38ZB3W_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'mean_train_score'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-65f5f67aefe4>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Extract the mean test scores for each cross-validation fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmean_test_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_random_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmean_train_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_random_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_train_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Extract the number of estimators for each iteration of the RandomizedSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'mean_train_score'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}